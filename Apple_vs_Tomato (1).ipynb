{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Link:https://www.kaggle.com/datasets/samuelcortinhas/apples-or-tomatoes-image-classification"
      ],
      "metadata": {
        "id": "VbXlJ8t58_B-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import load_img,img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import image_dataset_from_directory"
      ],
      "metadata": {
        "id": "uv4b5Rk8Ge_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "tmzM1ARiKDD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355240cf-92a6-4eb2-e80b-5ff215626490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds=image_dataset_from_directory(\n",
        "    directory='/content/drive/MyDrive/Colab Notebooks/Datasets/Apple-vs-Tomato/train',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=(256,256)\n",
        ")\n",
        "val_ds=image_dataset_from_directory(\n",
        "    directory='/content/drive/MyDrive/Colab Notebooks/Datasets/Apple-vs-Tomato/test',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=(256,256)\n",
        ")"
      ],
      "metadata": {
        "id": "c1t66S2LIBnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1479dbfb-7533-4550-c090-e254a3e26637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 294 files belonging to 2 classes.\n",
            "Found 97 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "NEFOBNTIVhA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen=ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "val_datagen=ImageDataGenerator(rescale=1./255)\n",
        "train_generator=train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Datasets/Apple-vs-Tomato/train',\n",
        "    target_size=(256,256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "val_generator=train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Datasets/Apple-vs-Tomato/test',\n",
        "    target_size=(256,256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgTQFPtDVlMU",
        "outputId": "5b002ed8-79c7-4c96-df27-b571067c3866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 294 images belonging to 2 classes.\n",
            "Found 97 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "PnJYmAlaraRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "conv_base = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top = False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n",
        "conv_base.trainable = False"
      ],
      "metadata": {
        "id": "9FOyOaTOriYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "0vnbiInJsGF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(train_generator,epochs=10,validation_data=val_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUDj9KrEsw5l",
        "outputId": "379501c9-0c51-4b91-fe55-0c931237d89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 9s 704ms/step - loss: 1.2859 - accuracy: 0.5442 - val_loss: 0.6371 - val_accuracy: 0.7113\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 6s 647ms/step - loss: 0.4747 - accuracy: 0.7823 - val_loss: 0.5541 - val_accuracy: 0.7526\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 8s 797ms/step - loss: 0.3431 - accuracy: 0.8810 - val_loss: 0.3971 - val_accuracy: 0.8351\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 7s 755ms/step - loss: 0.2550 - accuracy: 0.9218 - val_loss: 0.3756 - val_accuracy: 0.8557\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 8s 802ms/step - loss: 0.1736 - accuracy: 0.9490 - val_loss: 0.3711 - val_accuracy: 0.8763\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 9s 888ms/step - loss: 0.1738 - accuracy: 0.9354 - val_loss: 0.3997 - val_accuracy: 0.8866\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 6s 641ms/step - loss: 0.1556 - accuracy: 0.9252 - val_loss: 0.3494 - val_accuracy: 0.8969\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 8s 803ms/step - loss: 0.0922 - accuracy: 0.9660 - val_loss: 0.4169 - val_accuracy: 0.8454\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 6s 633ms/step - loss: 0.0650 - accuracy: 0.9830 - val_loss: 0.3893 - val_accuracy: 0.8351\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 8s 832ms/step - loss: 0.0754 - accuracy: 0.9762 - val_loss: 0.3928 - val_accuracy: 0.8557\n"
          ]
        }
      ]
    }
  ]
}